{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chckpt_path = './models/'\n",
    "results_path = './results/'\n",
    "if os.path.exists(chckpt_path):\n",
    "    rmtree(chckpt_path)\n",
    "if os.path.exists(results_path):\n",
    "    rmtree(results_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/train/\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "data = pd.concat((pd.read_csv(f, sep=\"\\t\") for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test/000000000000.csv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = train_data.iloc[:,2:33]\n",
    "del cat_train[\"f_7\"] #Only one value --> useless\n",
    "bin_train = train_data.iloc[:,33:42]\n",
    "num_train = train_data.iloc[:,42:80]\n",
    "labels_train = train_data.iloc[:,80:82]\n",
    "\n",
    "# cat_val = val_data.iloc[:,2:33]\n",
    "# del cat_val[\"f_7\"] #Only one value --> useless\n",
    "# bin_val = val_data.iloc[:,33:42]\n",
    "# num_val = val_data.iloc[:,42:80]\n",
    "# labels_val = val_data.iloc[:,80:82]\n",
    "\n",
    "cat_test = test_data.iloc[:,2:33]\n",
    "del cat_test[\"f_7\"] #Only one value --> useless\n",
    "bin_test = test_data.iloc[:,33:42]\n",
    "num_test = test_data.iloc[:,42:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# y_val = labels_val\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# y_is_clicked = y.iloc[:,0]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_train_is_installed \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m y_val_is_installed \u001b[39m=\u001b[39m y_val\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Categorical variables\n",
    "cat_train_selected = cat_train.to_numpy()\n",
    "# cat_val_selected = cat_val.to_numpy()\n",
    "cat_test_selected = cat_test.to_numpy()\n",
    "\n",
    "# Numerical variables : estimate missing values and normalize \n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "num_train_selected = imputer.fit_transform(num_train)\n",
    "scaler = MinMaxScaler()\n",
    "num_train_selected = scaler.fit_transform(num_train_selected)\n",
    "\n",
    "# num_val_selected = scaler.transform(imputer.transform(num_val))\n",
    "num_test_selected = scaler.transform(imputer.transform(num_test))\n",
    "\n",
    "# Binary variables\n",
    "bin_train_selected = bin_train.to_numpy()\n",
    "# bin_val_selected = bin_val.to_numpy()\n",
    "bin_test_selected = bin_test.to_numpy()\n",
    "\n",
    "# Output variables\n",
    "y_train = labels_train\n",
    "# y_val = labels_val\n",
    "# y_is_clicked = y.iloc[:,0]\n",
    "y_train_is_installed = y_train.iloc[:,1]\n",
    "# y_val_is_installed = y_val.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_ind in range(cat_train_selected.shape[1]):\n",
    "\n",
    "    unique_values = np.unique(cat_train_selected[:, col_ind][~np.isnan(cat_train_selected[:,col_ind])]).astype(int)\n",
    "    # test_unique_values = np.unique(cat_test_selected[:, col_ind]).astype(int)\n",
    "\n",
    "    # Make categorical variables from 1 to n (n corresponding to the number of unique values for the corresponding categorical feature)\n",
    "    replacement_dict = dict()\n",
    "    for index, val in enumerate(unique_values):\n",
    "        index+=1\n",
    "        replacement_dict[val] = index\n",
    "\n",
    "    # Process training data (categorical)\n",
    "    for line_ind in range(len(cat_train_selected[:,col_ind])):\n",
    "        if math.isnan(cat_train_selected[line_ind, col_ind]): # 0 used for missing values\n",
    "            cat_train_selected[line_ind, col_ind] = 0\n",
    "        else:\n",
    "            cat_train_selected[line_ind, col_ind] = replacement_dict[int(cat_train_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "\n",
    "    # # Process validation data (categorical)\n",
    "    # for line_ind in range(len(cat_val_selected[:,col_ind])):\n",
    "    #     try:\n",
    "    #         if math.isnan(cat_val_selected[line_ind, col_ind]):\n",
    "    #             cat_val_selected[line_ind, col_ind] = 0 # 0 used for missing values\n",
    "    #         else:\n",
    "    #             cat_val_selected[line_ind, col_ind] = replacement_dict[int(cat_val_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "    #     except KeyError:\n",
    "    #         cat_val_selected[line_ind, col_ind] = 0 # If the value was not in the training data, treat as a missing value (because we can't train on it)\n",
    "    \n",
    "    # Process test data (categorical)\n",
    "    for line_ind in range(len(cat_test_selected[:,col_ind])):\n",
    "        try:\n",
    "            if math.isnan(cat_test_selected[line_ind, col_ind]):\n",
    "                cat_test_selected[line_ind, col_ind] = 0 # 0 used for missing values\n",
    "            else:\n",
    "                cat_test_selected[line_ind, col_ind] = replacement_dict[int(cat_test_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "        except KeyError:\n",
    "            cat_test_selected[line_ind, col_ind] = 0 # If the value was not in the training data, treat as a missing value (because we can't train on it)\n",
    "\n",
    "cat_train_selected = cat_train_selected.astype(int)\n",
    "# cat_val_selected = cat_val_selected.astype(int)\n",
    "cat_test_selected = cat_test_selected.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 3485852\n",
      "    Positive: 606602 (17.40% of total)\n",
      "\n",
      "[-1.55741223]\n"
     ]
    }
   ],
   "source": [
    "# Compute bias to help the model with imbalanced dataset\n",
    "neg, pos = np.bincount(y_train_is_installed) \n",
    "total = neg + pos \n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format( total, pos, 100 * pos / total)) \n",
    "initial_bias = np.log([pos/neg]) \n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "\n",
    "# Inputs\n",
    "cat_input_layer = layers.Input(shape=(cat_train_selected.shape[1],), dtype=tf.int64)\n",
    "bin_input_layer = layers.Input(shape=(bin_train_selected.shape[1],), dtype=tf.int64)\n",
    "num_input_layer = layers.Input(shape=(num_train_selected.shape[1],), dtype=tf.float64)\n",
    "\n",
    "embedding_layers = []\n",
    "for i in range(cat_train_selected.shape[1]):\n",
    "    num_values = len(set(cat_train_selected[:,i]))\n",
    "    if num_values <= embed_size:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=num_values, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "    else:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=embed_size, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "\n",
    "bin_dense_layer = layers.Dense(64, activation='relu')(bin_input_layer)\n",
    "num_dense_layer = layers.Dense(64, activation='relu')(num_input_layer)\n",
    "\n",
    "# Concat all inputs\n",
    "concatted = tf.keras.layers.Concatenate()([bin_dense_layer, num_dense_layer, *embedding_layers])\n",
    "\n",
    "# Hidden layers\n",
    "hidden_layer_1 = layers.Dense(500, activation='relu')(concatted)\n",
    "hidden_layer_2 = layers.Dense(250, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = layers.Dense(50, activation='relu')(hidden_layer_2)\n",
    "hidden_layer_4 = layers.Dense(100, activation='relu')(hidden_layer_3)\n",
    "hidden_layer_5 = layers.Dense(40, activation='relu')(hidden_layer_4)\n",
    "\n",
    "# Outputs\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "#output_1 = layers.Dense(1, activation='sigmoid', name=\"is_clicked\", bias_initializer=output_bias)(hidden_layer_5) # If we want to predict \"is_clicked\", use this output (give two outputs to the model instead of one).\n",
    "output_2 = layers.Dense(1, activation=\"sigmoid\", name=\"is_installed\", bias_initializer=output_bias)(hidden_layer_5)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_2])\n",
    "# model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_1, output_2]) # If we want to predict \"is_clicked\" and \"is_installed\"\n",
    "\n",
    "# Compile model\n",
    "batch_size = 5000 \n",
    "learning_rate=0.001\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "if len(model.outputs)>1:\n",
    "    monitor_name = 'val_is_installed_loss'\n",
    "else:\n",
    "    monitor_name = \"val_loss\"\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor_name, patience=3)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath= chckpt_path + '{epoch:04d}', save_best_only=False, save_weights_only=False, save_freq='epoch')\n",
    "\n",
    "acc = tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', acc])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, \"model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/698 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8494 - binary_accuracy: 0.8494INFO:tensorflow:Assets written to: ./models/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/0001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 54s 74ms/step - loss: 0.3499 - accuracy: 0.8494 - binary_accuracy: 0.8494\n",
      "Epoch 2/3\n",
      "697/698 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8635 - binary_accuracy: 0.8635INFO:tensorflow:Assets written to: ./models/0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/0002/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 54s 78ms/step - loss: 0.3174 - accuracy: 0.8635 - binary_accuracy: 0.8635\n",
      "Epoch 3/3\n",
      "697/698 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8669 - binary_accuracy: 0.8669INFO:tensorflow:Assets written to: ./models/0003/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/0003/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 54s 77ms/step - loss: 0.3088 - accuracy: 0.8669 - binary_accuracy: 0.8669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c6ce0f70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs = 3, batch_size=batch_size, callbacks=[mcp_save])\n",
    "\n",
    "# model.fit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs = 50, batch_size=batch_size, \n",
    "#           validation_data=((cat_val_selected, bin_val_selected, num_val_selected), y_val_is_installed),\n",
    "#           callbacks=[mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models= glob.glob(chckpt_path+\"/*\") # * means all if need specific format then *.csv\n",
    "latest_model = max(list_of_models, key=os.path.getctime)\n",
    "# print(latest_file)\n",
    "model = tf.keras.models.load_model(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    y_pred[y_pred>=0.5]=1\n",
    "    y_pred[y_pred<0.5]=0\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    tpr = round(tp / (tp+fn),4)\n",
    "    fpr = round(tp / (fp+tn),4)\n",
    "    tnr = round(tn / (tn+fp),4)\n",
    "    fnr = round(fn / (fn+tp),4)\n",
    "    acc = round((tp + tn) / (tp+fn+fp+tn),4)\n",
    "    precision = round(tp / (tp + fp),4)\n",
    "    f1 = round(2 * (precision * tpr) / (precision + tpr),4)\n",
    "    \n",
    "    return tpr, fpr, tnr, fnr, acc, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 16s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/974ggbdj74sczwq4lflylfy00000gn/T/ipykernel_13564/2816523072.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = round(tp / (tp + fp),4)\n"
     ]
    }
   ],
   "source": [
    "# # Val predictions\n",
    "# y_pred_val = model.predict((cat_val_selected, bin_val_selected, num_val_selected), batch_size=batch_size)\n",
    "# tpr_val, fpr_val, tnr_val, fnr_val, acc_val, precision_val, f1_val = compute_metrics(y_val_is_installed, y_pred_val)\n",
    "\n",
    "\n",
    "# # Val dumb predictions\n",
    "# y_pred_val_dumb = np.zeros(y_val_is_installed.shape)\n",
    "# tpr_val_dumb, fpr_val_dumb, tnr_val_dumb, fnr_val_dumb, acc_val_dumb, precision_val_dumb, f1_val_dumb = compute_metrics(y_val_is_installed, y_pred_val_dumb)\n",
    "\n",
    "# Train predictions\n",
    "y_pred_train = model.predict((cat_train_selected, bin_train_selected, num_train_selected), batch_size=batch_size)\n",
    "tpr_train, fpr_train, tnr_train, fnr_train, acc_train, precision_train, f1_train = compute_metrics(y_train_is_installed, y_pred_train)\n",
    "\n",
    "# Train dumb predictions\n",
    "y_pred_train_dumb = np.zeros(y_train_is_installed.shape)\n",
    "tpr_train_dumb, fpr_train_dumb, tnr_train_dumb, fnr_train_dumb, acc_train_dumb, precision_train_dumb, f1_train_dumb = compute_metrics(y_train_is_installed, y_pred_train_dumb)\n",
    "\n",
    "# FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "# FN = cm.sum(axis=1) - np.diag(cm)\n",
    "# TP = np.diag(cm)\n",
    "# TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "# # Sensitivity, hit rate, recall, or true positive rate\n",
    "# TPR = TP/(TP+FN)\n",
    "# # Specificity or true negative rate\n",
    "# TNR = TN/(TN+FP) \n",
    "# # Precision or positive predictive value\n",
    "# PPV = TP/(TP+FP)\n",
    "# # Negative predictive value\n",
    "# NPV = TN/(TN+FN)\n",
    "# # Fall out or false positive rate\n",
    "# FPR = FP/(FP+TN)\n",
    "# # False negative rate\n",
    "# FNR = FN/(TP+FN)\n",
    "# # False discovery rate\n",
    "# FDR = FP/(TP+FP)\n",
    "\n",
    "# # Overall accuracy\n",
    "# ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5318, 0.112, 0.8888, 0.4682, 0.8266, 0.5018, 0.5164, 0.826)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_train, fpr_train, tnr_train, fnr_train, acc_train, precision_train, f1_train, acc_train_dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 23ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (160973)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdel\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mf_0\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39m# results[\"is_clicked\"] = 0 # Not predicted here, but can be predicted if the model is configured with two outputs\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results[\u001b[39m\"\u001b[39;49m\u001b[39mis_clicked\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m y_test[\u001b[39m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m results[\u001b[39m\"\u001b[39m\u001b[39mis_installed\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_test[\u001b[39m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m save_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4145\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/pandas/core/frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4869\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4870\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4871\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (1) does not match length of index (160973)"
     ]
    }
   ],
   "source": [
    "# y_test = model.predict((cat_test_selected, bin_test_selected, num_test_selected), batch_size=batch_size) \n",
    "\n",
    "# results = pd.DataFrame(test_data[\"f_0\"].copy())\n",
    "# results[\"row_id\"] = results[\"f_0\"]\n",
    "# del results[\"f_0\"]\n",
    "\n",
    "# # results[\"is_clicked\"] = 0 # Not predicted here, but can be predicted if the model is configured with two outputs\n",
    "# results[\"is_clicked\"] = y_test[0]\n",
    "# results[\"is_installed\"] = y_test[1]\n",
    "\n",
    "# save_path=\"results/\"\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.makedirs(save_path)\n",
    "# results.to_csv(os.path.join(save_path, \"test_predictions\" + \".csv\"), index=False, header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
