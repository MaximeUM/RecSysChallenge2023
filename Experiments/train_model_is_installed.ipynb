{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chckpt_path = './models/'\n",
    "results_path = './results/'\n",
    "if os.path.exists(chckpt_path):\n",
    "    rmtree(chckpt_path)\n",
    "if os.path.exists(results_path):\n",
    "    rmtree(results_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/train/\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "data = pd.concat((pd.read_csv(f, sep=\"\\t\") for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test/000000000000.csv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = train_data.iloc[:,2:33]\n",
    "del cat_train[\"f_7\"] #Only one value --> useless\n",
    "bin_train = train_data.iloc[:,33:42]\n",
    "num_train = train_data.iloc[:,42:80]\n",
    "labels_train = train_data.iloc[:,80:82]\n",
    "\n",
    "cat_val = val_data.iloc[:,2:33]\n",
    "del cat_val[\"f_7\"] #Only one value --> useless\n",
    "bin_val = val_data.iloc[:,33:42]\n",
    "num_val = val_data.iloc[:,42:80]\n",
    "labels_val = val_data.iloc[:,80:82]\n",
    "\n",
    "cat_test = test_data.iloc[:,2:33]\n",
    "del cat_test[\"f_7\"] #Only one value --> useless\n",
    "bin_test = test_data.iloc[:,33:42]\n",
    "num_test = test_data.iloc[:,42:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cat_train_selected = cat_train.to_numpy()\n",
    "cat_val_selected = cat_val.to_numpy()\n",
    "cat_test_selected = cat_test.to_numpy()\n",
    "\n",
    "# Numerical variables : estimate missing values and normalize \n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "num_train_selected = imputer.fit_transform(num_train)\n",
    "scaler = MinMaxScaler()\n",
    "num_train_selected = scaler.fit_transform(num_train_selected)\n",
    "\n",
    "num_val_selected = scaler.transform(imputer.transform(num_val))\n",
    "num_test_selected = scaler.transform(imputer.transform(num_test))\n",
    "\n",
    "# Binary variables\n",
    "bin_train_selected = bin_train.to_numpy()\n",
    "bin_val_selected = bin_val.to_numpy()\n",
    "bin_test_selected = bin_test.to_numpy()\n",
    "\n",
    "# Output variables\n",
    "y_train = labels_train\n",
    "y_val = labels_val\n",
    "# y_is_clicked = y.iloc[:,0]\n",
    "y_train_is_installed = y_train.iloc[:,1]\n",
    "y_val_is_installed = y_val.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_ind in range(cat_train_selected.shape[1]):\n",
    "\n",
    "    unique_values = np.unique(cat_train_selected[:, col_ind][~np.isnan(cat_train_selected[:,col_ind])]).astype(int)\n",
    "    # test_unique_values = np.unique(cat_test_selected[:, col_ind]).astype(int)\n",
    "\n",
    "    # Make categorical variables from 1 to n (n corresponding to the number of unique values for the corresponding categorical feature)\n",
    "    replacement_dict = dict()\n",
    "    for index, val in enumerate(unique_values):\n",
    "        index+=1\n",
    "        replacement_dict[val] = index\n",
    "\n",
    "    # Process training data (categorical)\n",
    "    for line_ind in range(len(cat_train_selected[:,col_ind])):\n",
    "        if math.isnan(cat_train_selected[line_ind, col_ind]): # 0 used for missing values\n",
    "            cat_train_selected[line_ind, col_ind] = 0\n",
    "        else:\n",
    "            cat_train_selected[line_ind, col_ind] = replacement_dict[int(cat_train_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "\n",
    "    # Process validation data (categorical)\n",
    "    for line_ind in range(len(cat_val_selected[:,col_ind])):\n",
    "        try:\n",
    "            if math.isnan(cat_val_selected[line_ind, col_ind]):\n",
    "                cat_val_selected[line_ind, col_ind] = 0 # 0 used for missing values\n",
    "            else:\n",
    "                cat_val_selected[line_ind, col_ind] = replacement_dict[int(cat_val_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "        except KeyError:\n",
    "            cat_val_selected[line_ind, col_ind] = 0 # If the value was not in the training data, treat as a missing value (because we can't train on it)\n",
    "    \n",
    "    # Process test data (categorical)\n",
    "    for line_ind in range(len(cat_test_selected[:,col_ind])):\n",
    "        try:\n",
    "            if math.isnan(cat_test_selected[line_ind, col_ind]):\n",
    "                cat_test_selected[line_ind, col_ind] = 0 # 0 used for missing values\n",
    "            else:\n",
    "                cat_test_selected[line_ind, col_ind] = replacement_dict[int(cat_test_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "        except KeyError:\n",
    "            cat_test_selected[line_ind, col_ind] = 0 # If the value was not in the training data, treat as a missing value (because we can't train on it)\n",
    "\n",
    "cat_train_selected = cat_train_selected.astype(int)\n",
    "cat_val_selected = cat_val_selected.astype(int)\n",
    "cat_test_selected = cat_test_selected.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 2614389\n",
      "    Positive: 455360 (17.42% of total)\n",
      "\n",
      "[-1.55632555]\n"
     ]
    }
   ],
   "source": [
    "# Compute bias to help the model with imbalanced dataset\n",
    "neg, pos = np.bincount(y_train_is_installed) \n",
    "total = neg + pos \n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format( total, pos, 100 * pos / total)) \n",
    "initial_bias = np.log([pos/neg]) \n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.backend as K\n",
    "# tp = tf.keras.metrics.TruePositives()\n",
    "# fp = tf.keras.metrics.FalsePositives()\n",
    "\n",
    "# def pos(y_true, y_pred):\n",
    "#     fp = tf.keras.metrics.FalsePositives()\n",
    "#     fp.update_state(bigy[test], pred)\n",
    "#     fp = fp.result().numpy()\n",
    "\n",
    "#     fn = tf.keras.metrics.FalseNegatives()\n",
    "#     fn.update_state(bigy[test], pred)\n",
    "#     fn = fn.result().numpy()\n",
    "\n",
    "#     tn = tf.keras.metrics.TrueNegatives()\n",
    "#     tn.update_state(bigy[test], pred)\n",
    "#     tn = tn.result().numpy()\n",
    "\n",
    "# def neg():\n",
    "#     return tf.keras.metrics.TrueNegatives(y_true, y_pred) + tf.keras.metrics.FalseNegatives(y_true, y_pred)\n",
    "\n",
    "# # def neg(y_true, y_pred):\n",
    "# #     return K.sum(y_true[y_true==0], axis=0)\n",
    "\n",
    "# def fpr(y_true, y_pred):\n",
    "#     return K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0) #/ pos(y_true, y_pred)\n",
    "\n",
    "# def fnr(y_true, y_pred):\n",
    "#     return K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0) / neg(y_true, y_pred)\n",
    "\n",
    "# def tpr(y_true, y_pred):\n",
    "#     return K.sum(K.cast(y_true*y_pred, 'float'), axis=0) / pos(y_true, y_pred)\n",
    "\n",
    "# def tnr(y_true, y_pred):\n",
    "#     return K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0) / neg(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "\n",
    "# Inputs\n",
    "cat_input_layer = layers.Input(shape=(cat_train_selected.shape[1],), dtype=tf.int64)\n",
    "bin_input_layer = layers.Input(shape=(bin_train_selected.shape[1],), dtype=tf.int64)\n",
    "num_input_layer = layers.Input(shape=(num_train_selected.shape[1],), dtype=tf.float64)\n",
    "\n",
    "embedding_layers = []\n",
    "for i in range(cat_train_selected.shape[1]):\n",
    "    num_values = len(set(cat_train_selected[:,i]))\n",
    "    if num_values <= embed_size:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=num_values, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "    else:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=embed_size, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "\n",
    "bin_dense_layer = layers.Dense(64, activation='relu')(bin_input_layer)\n",
    "num_dense_layer = layers.Dense(64, activation='relu')(num_input_layer)\n",
    "\n",
    "# Concat all inputs\n",
    "concatted = tf.keras.layers.Concatenate()([bin_dense_layer, num_dense_layer, *embedding_layers])\n",
    "\n",
    "# Hidden layers\n",
    "hidden_layer_1 = layers.Dense(500, activation='relu')(concatted)\n",
    "hidden_layer_2 = layers.Dense(250, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = layers.Dense(50, activation='relu')(hidden_layer_2)\n",
    "hidden_layer_4 = layers.Dense(100, activation='relu')(hidden_layer_3)\n",
    "hidden_layer_5 = layers.Dense(40, activation='relu')(hidden_layer_4)\n",
    "\n",
    "# Outputs\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "#output_1 = layers.Dense(1, activation='sigmoid', name=\"is_clicked\", bias_initializer=output_bias)(hidden_layer_5) # If we want to predict \"is_clicked\", use this output (give two outputs to the model instead of one).\n",
    "output_2 = layers.Dense(1, activation=\"sigmoid\", name=\"is_installed\", bias_initializer=output_bias)(hidden_layer_5)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_2])\n",
    "# model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_1, output_2]) # If we want to predict \"is_clicked\" and \"is_installed\"\n",
    "\n",
    "# Compile model\n",
    "batch_size = 5000 \n",
    "learning_rate=0.001\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "if len(model.outputs)>1:\n",
    "    monitor_name = 'val_is_installed_loss'\n",
    "else:\n",
    "    monitor_name = \"val_loss\"\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor_name, patience=3)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath= chckpt_path + '{epoch:04d}', save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min', save_freq='epoch')\n",
    "# mcp_save = tf.keras.callbacks.ModelCheckpoint(filepath= chckpt_path + '{epoch:04d}', save_best_only=False, save_weights_only=False, monitor='val_loss', mode='min', save_freq='epoch')\n",
    "\n",
    "acc = tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', acc])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, \"model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "523/523 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8461 - binary_accuracy: 0.8461INFO:tensorflow:Assets written to: ./models/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/0001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/523 [==============================] - 45s 82ms/step - loss: 0.3569 - accuracy: 0.8461 - binary_accuracy: 0.8461 - val_loss: 0.3287 - val_accuracy: 0.8582 - val_binary_accuracy: 0.8582\n",
      "Epoch 2/50\n",
      "523/523 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8623 - binary_accuracy: 0.8623INFO:tensorflow:Assets written to: ./models/0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/0002/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/523 [==============================] - 43s 82ms/step - loss: 0.3199 - accuracy: 0.8623 - binary_accuracy: 0.8623 - val_loss: 0.3195 - val_accuracy: 0.8636 - val_binary_accuracy: 0.8636\n",
      "Epoch 3/50\n",
      "523/523 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.8660 - binary_accuracy: 0.8660"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m      2\u001b[0m           validation_data\u001b[39m=\u001b[39;49m((cat_val_selected, bin_val_selected, num_val_selected), y_val_is_installed),\n\u001b[1;32m      3\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[early_stopping, mcp_save])\n\u001b[1;32m      5\u001b[0m \u001b[39m# model.fit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs = 50, batch_size=batch_size, \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#           validation_data=((cat_val_selected, bin_val_selected, num_val_selected), y_val_is_installed),\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#           callbacks=[mcp_save])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/engine/training.py:1809\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m     }\n\u001b[1;32m   1807\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1809\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1810\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/callbacks.py:453\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    451\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    452\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 453\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/callbacks.py:1483\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs_since_last_save \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_freq \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1483\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_model(epoch\u001b[39m=\u001b[39;49mepoch, batch\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/callbacks.py:1553\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_weights(\n\u001b[1;32m   1548\u001b[0m             filepath,\n\u001b[1;32m   1549\u001b[0m             overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1550\u001b[0m             options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options,\n\u001b[1;32m   1551\u001b[0m         )\n\u001b[1;32m   1552\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m   1554\u001b[0m             filepath,\n\u001b[1;32m   1555\u001b[0m             overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1556\u001b[0m             options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options,\n\u001b[1;32m   1557\u001b[0m         )\n\u001b[1;32m   1558\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/engine/training.py:3000\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[1;32m   2947\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, filepath, overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_format\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2948\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Saves a model as a TensorFlow SavedModel or HDF5 file.\u001b[39;00m\n\u001b[1;32m   2949\u001b[0m \n\u001b[1;32m   2950\u001b[0m \u001b[39m    See the [Serialization and Saving guide](\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[39m    Note that `model.save()` is an alias for `tf.keras.models.save_model()`.\u001b[39;00m\n\u001b[1;32m   2999\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3000\u001b[0m     saving_api\u001b[39m.\u001b[39;49msave_model(\n\u001b[1;32m   3001\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3002\u001b[0m         filepath\u001b[39m=\u001b[39;49mfilepath,\n\u001b[1;32m   3003\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m   3004\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m   3005\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3006\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/saving/saving_api.py:149\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     saving_lib\u001b[39m.\u001b[39msave_model(model, filepath)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39m# Legacy case\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49msave_model(\n\u001b[1;32m    150\u001b[0m         model,\n\u001b[1;32m    151\u001b[0m         filepath,\n\u001b[1;32m    152\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    153\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m    154\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    155\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/saving/legacy/save.py:168\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m serialization\u001b[39m.\u001b[39mSharedObjectSavingScope():\n\u001b[1;32m    165\u001b[0m     \u001b[39mwith\u001b[39;00m keras_option_scope(\n\u001b[1;32m    166\u001b[0m         save_traces\u001b[39m=\u001b[39msave_traces, in_tf_saved_model_scope\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     ):\n\u001b[0;32m--> 168\u001b[0m         saved_model_save\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m    169\u001b[0m             model,\n\u001b[1;32m    170\u001b[0m             filepath,\n\u001b[1;32m    171\u001b[0m             overwrite,\n\u001b[1;32m    172\u001b[0m             include_optimizer,\n\u001b[1;32m    173\u001b[0m             signatures,\n\u001b[1;32m    174\u001b[0m             options,\n\u001b[1;32m    175\u001b[0m             save_traces,\n\u001b[1;32m    176\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/keras/src/saving/legacy/saved_model/save.py:98\u001b[0m, in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mdeprecated_internal_learning_phase_scope(\u001b[39m0\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mkeras_option_scope(save_traces):\n\u001b[0;32m---> 98\u001b[0m         saved_nodes, node_paths \u001b[39m=\u001b[39m save_lib\u001b[39m.\u001b[39;49msave_and_return_nodes(\n\u001b[1;32m     99\u001b[0m             model, filepath, signatures, options\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    102\u001b[0m     \u001b[39m# Save all metadata to a separate file in the SavedModel directory.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     metadata \u001b[39m=\u001b[39m generate_keras_metadata(saved_nodes, node_paths)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1313\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1309\u001b[0m saved_model \u001b[39m=\u001b[39m saved_model_pb2\u001b[39m.\u001b[39mSavedModel()\n\u001b[1;32m   1310\u001b[0m meta_graph_def \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mmeta_graphs\u001b[39m.\u001b[39madd()\n\u001b[1;32m   1312\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1313\u001b[0m     _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001b[1;32m   1314\u001b[0m saved_model\u001b[39m.\u001b[39msaved_model_schema_version \u001b[39m=\u001b[39m (\n\u001b[1;32m   1315\u001b[0m     constants\u001b[39m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1317\u001b[0m \u001b[39m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[39m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1493\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \n\u001b[1;32m   1468\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \u001b[39mwith\u001b[39;00m save_context\u001b[39m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1493\u001b[0m   \u001b[39mreturn\u001b[39;00m _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1437\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1435\u001b[0m saveable_view \u001b[39m=\u001b[39m _SaveableView(augmented_graph_view, options)\n\u001b[1;32m   1436\u001b[0m object_saver \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mTrackableSaver(augmented_graph_view)\n\u001b[0;32m-> 1437\u001b[0m asset_info, exported_graph \u001b[39m=\u001b[39m _fill_meta_graph_def(\n\u001b[1;32m   1438\u001b[0m     meta_graph_def,\n\u001b[1;32m   1439\u001b[0m     saveable_view,\n\u001b[1;32m   1440\u001b[0m     signatures,\n\u001b[1;32m   1441\u001b[0m     options\u001b[39m.\u001b[39;49mnamespace_whitelist,\n\u001b[1;32m   1442\u001b[0m     options\u001b[39m.\u001b[39;49mexperimental_custom_gradients,\n\u001b[1;32m   1443\u001b[0m     defaults,\n\u001b[1;32m   1444\u001b[0m )\n\u001b[1;32m   1445\u001b[0m \u001b[39mif\u001b[39;00m options\u001b[39m.\u001b[39mfunction_aliases:\n\u001b[1;32m   1446\u001b[0m   function_aliases \u001b[39m=\u001b[39m meta_graph_def\u001b[39m.\u001b[39mmeta_info_def\u001b[39m.\u001b[39mfunction_aliases\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:901\u001b[0m, in \u001b[0;36m_fill_meta_graph_def\u001b[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist, save_custom_gradients, defaults)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[39m# At this point all nodes that can be added to the SavedObjectGraph have been\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m# added, so run the following to validate deserialization dependencies.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m _dependency_sorted_node_ids(saveable_view)\n\u001b[0;32m--> 901\u001b[0m graph_def \u001b[39m=\u001b[39m exported_graph\u001b[39m.\u001b[39;49mas_graph_def(add_shapes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    902\u001b[0m graph_def\u001b[39m.\u001b[39mlibrary\u001b[39m.\u001b[39mregistered_gradients\u001b[39m.\u001b[39mextend(saveable_view\u001b[39m.\u001b[39mgradient_defs)\n\u001b[1;32m    903\u001b[0m _verify_ops(graph_def, namespace_whitelist)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3164\u001b[0m, in \u001b[0;36mGraph.as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a serialized `GraphDef` representation of this graph.\u001b[39;00m\n\u001b[1;32m   3141\u001b[0m \n\u001b[1;32m   3142\u001b[0m \u001b[39mThe serialized `GraphDef` can be imported into another `Graph`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[39m  ValueError: If the `graph_def` would be too large.\u001b[39;00m\n\u001b[1;32m   3162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m-> 3164\u001b[0m result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_def(from_version, add_shapes)\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3078\u001b[0m, in \u001b[0;36mGraph._as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3076\u001b[0m     data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n\u001b[1;32m   3077\u001b[0m graph \u001b[39m=\u001b[39m graph_pb2\u001b[39m.\u001b[39mGraphDef()\n\u001b[0;32m-> 3078\u001b[0m graph\u001b[39m.\u001b[39;49mParseFromString(compat\u001b[39m.\u001b[39;49mas_bytes(data))\n\u001b[1;32m   3079\u001b[0m \u001b[39m# Strip the experimental library field iff it's empty.\u001b[39;00m\n\u001b[1;32m   3080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m graph\u001b[39m.\u001b[39mlibrary\u001b[39m.\u001b[39mfunction:\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/message.py:202\u001b[0m, in \u001b[0;36mMessage.ParseFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse serialized protocol buffer data into this message.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[39mLike :func:`MergeFromString()`, except we clear the object first.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m  message.DecodeError if the input cannot be parsed.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mClear()\n\u001b[0;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMergeFromString(serialized)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1128\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.MergeFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m   1126\u001b[0m length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(serialized)\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_InternalParse(serialized, \u001b[39m0\u001b[39;49m, length) \u001b[39m!=\u001b[39m length:\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m message_mod\u001b[39m.\u001b[39mDecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1132\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m   1133\u001b[0m   \u001b[39m# Now ord(buf[p:p+1]) == ord('') gets TypeError.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/decoder.py:726\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    724\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    725\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39;49m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    727\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it encountered\u001b[39;00m\n\u001b[1;32m    728\u001b[0m   \u001b[39m# an end-group tag.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m new_pos\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/decoder.py:705\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeRepeatedField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39;49madd()\u001b[39m.\u001b[39;49m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    706\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    709\u001b[0m \u001b[39m# Predict that the next tag is another copy of the same repeated field.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/decoder.py:705\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeRepeatedField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39;49madd()\u001b[39m.\u001b[39;49m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    706\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    709\u001b[0m \u001b[39m# Predict that the next tag is another copy of the same repeated field.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/decoder.py:859\u001b[0m, in \u001b[0;36mMapDecoder.<locals>.DecodeMap\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    857\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    858\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m submsg\u001b[39m.\u001b[39;49mClear()\n\u001b[1;32m    860\u001b[0m \u001b[39mif\u001b[39;00m submsg\u001b[39m.\u001b[39m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    861\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m    862\u001b[0m   \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/recsys2023/lib/python3.8/site-packages/google/protobuf/internal/python_message.py:1374\u001b[0m, in \u001b[0;36m_Clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Clear\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1373\u001b[0m   \u001b[39m# Clear fields.\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fields \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1375\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields \u001b[39m=\u001b[39m ()\n\u001b[1;32m   1376\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs = 50, batch_size=batch_size, \n",
    "          validation_data=((cat_val_selected, bin_val_selected, num_val_selected), y_val_is_installed),\n",
    "          callbacks=[early_stopping, mcp_save])\n",
    "\n",
    "# model.fit((cat_train_selected, bin_train_selected, num_train_selected), y_train_is_installed, epochs = 50, batch_size=batch_size, \n",
    "#           validation_data=((cat_val_selected, bin_val_selected, num_val_selected), y_val_is_installed),\n",
    "#           callbacks=[mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models= glob.glob(chckpt_path+\"/*\") # * means all if need specific format then *.csv\n",
    "latest_model = max(list_of_models, key=os.path.getctime)\n",
    "# print(latest_file)\n",
    "model = tf.keras.models.load_model(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 5s 25ms/step - loss: 0.3177 - accuracy: 0.8646 - binary_accuracy: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3177054226398468, 0.8645771145820618, 0.8645771145820618]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate((cat_val_selected, bin_val_selected, num_val_selected), np.expand_dims(y_val_is_installed.values.astype('float32'), axis=1), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 4s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict((cat_val_selected, bin_val_selected, num_val_selected), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.82069117>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba = tf.metrics.BinaryAccuracy()\n",
    "ba(np.expand_dims(y_val_is_installed.values, axis=1),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 4s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8206911825286902"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_val_is_installed\n",
    "y_pred = model.predict((cat_val_selected, bin_val_selected, num_val_selected), batch_size=batch_size)\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "y_pred=np.squeeze(y_pred)\n",
    "y_true=np.squeeze(y_true.values.astype('float32'))\n",
    "np.sum(np.equal(y_pred,y_true))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8264504631866184"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_true==0)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8258254605569408"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train_is_installed==0)/len(y_train_is_installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/523 [==============================] - 12s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.82366127>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train=model.predict((cat_train_selected,bin_train_selected,num_train_selected), batch_size=batch_size)\n",
    "ba = tf.metrics.BinaryAccuracy()\n",
    "ba(np.expand_dims(y_train_is_installed.values, axis=1),y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    y_pred[y_pred>=0.5]=1\n",
    "    y_pred[y_pred<0.5]=0\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    tpr = round(tp / (tp+fn),4)\n",
    "    fpr = round(tp / (fp+tn),4)\n",
    "    tnr = round(tn / (tn+fp),4)\n",
    "    fnr = round(fn / (fn+tp),4)\n",
    "    acc = round((tp + tn) / (tp+fn+fp+tn),4)\n",
    "    precision = round(tp / (tp + fp),4)\n",
    "    f1 = round(2 * (precision * tpr) / (precision + tpr),4)\n",
    "    \n",
    "    return tpr, fpr, tnr, fnr, acc, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 4s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/974ggbdj74sczwq4lflylfy00000gn/T/ipykernel_12547/2816523072.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = round(tp / (tp + fp),4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523/523 [==============================] - 12s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/974ggbdj74sczwq4lflylfy00000gn/T/ipykernel_12547/2816523072.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = round(tp / (tp + fp),4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Val predictions\n",
    "y_pred_val = model.predict((cat_val_selected, bin_val_selected, num_val_selected), batch_size=batch_size)\n",
    "tpr_val, fpr_val, tnr_val, fnr_val, acc_val, precision_val, f1_val = compute_metrics(y_val_is_installed, y_pred_val)\n",
    "\n",
    "\n",
    "# Val dumb predictions\n",
    "y_pred_val_dumb = np.zeros(y_val_is_installed.shape)\n",
    "tpr_val_dumb, fpr_val_dumb, tnr_val_dumb, fnr_val_dumb, acc_val_dumb, precision_val_dumb, f1_val_dumb = compute_metrics(y_val_is_installed, y_pred_val_dumb)\n",
    "\n",
    "# Train predictions\n",
    "y_pred_train = model.predict((cat_train_selected, bin_train_selected, num_train_selected), batch_size=batch_size)\n",
    "tpr_train, fpr_train, tnr_train, fnr_train, acc_train, precision_train, f1_train = compute_metrics(y_train_is_installed, y_pred_train)\n",
    "\n",
    "# Train dumb predictions\n",
    "y_pred_train_dumb = np.zeros(y_train_is_installed.shape)\n",
    "tpr_train_dumb, fpr_train_dumb, tnr_train_dumb, fnr_train_dumb, acc_train_dumb, precision_train_dumb, f1_train_dumb = compute_metrics(y_train_is_installed, y_pred_train_dumb)\n",
    "\n",
    "# FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "# FN = cm.sum(axis=1) - np.diag(cm)\n",
    "# TP = np.diag(cm)\n",
    "# TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "# # Sensitivity, hit rate, recall, or true positive rate\n",
    "# TPR = TP/(TP+FN)\n",
    "# # Specificity or true negative rate\n",
    "# TNR = TN/(TN+FP) \n",
    "# # Precision or positive predictive value\n",
    "# PPV = TP/(TP+FP)\n",
    "# # Negative predictive value\n",
    "# NPV = TN/(TN+FN)\n",
    "# # Fall out or false positive rate\n",
    "# FPR = FP/(FP+TN)\n",
    "# # False negative rate\n",
    "# FNR = FN/(TP+FN)\n",
    "# # False discovery rate\n",
    "# FDR = FP/(TP+FP)\n",
    "\n",
    "# # Overall accuracy\n",
    "# ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5324, 0.1118, 0.8812, 0.4676, 0.8207, 0.4849, 0.5075)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_val, fpr_val, tnr_val, fnr_val, acc_val, precision_val, f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5425, 0.1144, 0.883, 0.4575, 0.8237, 0.4943, 0.5173, 0.8258)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_train, fpr_train, tnr_train, fnr_train, acc_train, precision_train, f1_train, acc_train_dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.8265, nan, nan)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_val_dumb, tnr_val_dumb, acc_val_dumb, precision_val_dumb, f1_val_dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5324 0.4676 0.8812 0.1118 0.8207\n",
      "0.0 1.0 1.0 0.0 0.8265\n",
      "0.5425 0.4575 0.883 0.1144 0.8237\n",
      "0.0 1.0 1.0 0.0 0.8258\n"
     ]
    }
   ],
   "source": [
    "print(tpr_val, fnr_val, tnr_val, fpr_val, acc_val)\n",
    "print(tpr_val_dumb, fnr_val_dumb, tnr_val_dumb, fpr_val_dumb, acc_val_dumb)\n",
    "print(tpr_train, fnr_train, tnr_train, fpr_train, acc_train)\n",
    "print(tpr_train_dumb, fnr_train_dumb, tnr_train_dumb, fpr_train_dumb, acc_train_dumb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
