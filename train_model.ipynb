{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/train/\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "data = pd.concat((pd.read_csv(f, sep=\"\\t\") for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test/000000000000.csv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = data.iloc[:,2:33]\n",
    "del cat[\"f_7\"] #Only one value --> useless\n",
    "bin = data.iloc[:,33:42]\n",
    "num = data.iloc[:,42:80]\n",
    "labels = data.iloc[:,80:82]\n",
    "\n",
    "cat_test = test_data.iloc[:,2:33]\n",
    "del cat_test[\"f_7\"] #Only one value --> useless\n",
    "bin_test = test_data.iloc[:,33:42]\n",
    "num_test = test_data.iloc[:,42:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cat_selected = cat.to_numpy()\n",
    "cat_test_selected = cat_test.to_numpy()\n",
    "\n",
    "# Numerical variables : estimate missing values and normalize \n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "num_selected = imputer.fit_transform(num)\n",
    "scaler = MinMaxScaler()\n",
    "num_selected = scaler.fit_transform(num_selected)\n",
    "\n",
    "num_test_selected = scaler.transform(imputer.transform(num_test))\n",
    "\n",
    "# Binary variables\n",
    "bin_selected = bin.to_numpy()\n",
    "bin_test_selected = bin_test.to_numpy()\n",
    "\n",
    "# Output variables\n",
    "y = labels\n",
    "# y_is_clicked = y.iloc[:,0]\n",
    "y_is_installed = y.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_ind in range(cat_selected.shape[1]):\n",
    "\n",
    "    unique_values = np.unique(cat_selected[:, col_ind][~np.isnan(cat_selected[:,col_ind])]).astype(int)\n",
    "    # test_unique_values = np.unique(cat_test_selected[:, col_ind]).astype(int)\n",
    "\n",
    "    # Make categorical variables from 1 to n (n corresponding to the number of unique values for the corresponding categorical feature)\n",
    "    replacement_dict = dict()\n",
    "    for index, val in enumerate(unique_values):\n",
    "        index+=1\n",
    "        replacement_dict[val] = index\n",
    "\n",
    "    # Process training data (categorical)\n",
    "    for line_ind in range(len(cat_selected[:,col_ind])):\n",
    "        if math.isnan(cat_selected[line_ind, col_ind]): # 0 used for missing values\n",
    "            cat_selected[line_ind, col_ind] = 0\n",
    "        else:\n",
    "            cat_selected[line_ind, col_ind] = replacement_dict[int(cat_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "    \n",
    "    # Process test data (categorical)\n",
    "    for line_ind in range(len(cat_test_selected[:,col_ind])):\n",
    "        try:\n",
    "            if math.isnan(cat_test_selected[line_ind, col_ind]):\n",
    "                cat_test_selected[line_ind, col_ind] = 0 # 0 used for missing values\n",
    "            else:\n",
    "                cat_test_selected[line_ind, col_ind] = replacement_dict[int(cat_test_selected[line_ind, col_ind])] # Use the new value (from 1 to n)\n",
    "        except KeyError:\n",
    "            cat_test_selected[line_ind, col_ind] = 0 # If the value was not in the training data, treat as a missing value (because we can't train on it)\n",
    "\n",
    "cat_selected = cat_selected.astype(int)\n",
    "cat_test_selected = cat_test_selected.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bias to help the model with imbalanced dataset\n",
    "neg, pos = np.bincount(y_is_installed) \n",
    "total = neg + pos \n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format( total, pos, 100 * pos / total)) \n",
    "initial_bias = np.log([pos/neg]) \n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "\n",
    "# Inputs\n",
    "cat_input_layer = layers.Input(shape=(cat_selected.shape[1],), dtype=tf.int64)\n",
    "bin_input_layer = layers.Input(shape=(bin_selected.shape[1],), dtype=tf.int64)\n",
    "num_input_layer = layers.Input(shape=(num_selected.shape[1],), dtype=tf.float64)\n",
    "\n",
    "embedding_layers = []\n",
    "for i in range(cat_selected.shape[1]):\n",
    "    num_values = len(set(cat_selected[:,i]))\n",
    "    if num_values <= embed_size:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=num_values, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "    else:\n",
    "        embedding_layers.append(layers.Embedding(input_dim=num_values+1, output_dim=embed_size, input_length=1, mask_zero=True)(cat_input_layer[:,i]))\n",
    "\n",
    "bin_dense_layer = layers.Dense(64, activation='relu')(bin_input_layer)\n",
    "num_dense_layer = layers.Dense(64, activation='relu')(num_input_layer)\n",
    "\n",
    "# Concat all inputs\n",
    "concatted = tf.keras.layers.Concatenate()([bin_dense_layer, num_dense_layer, *embedding_layers])\n",
    "\n",
    "# Hidden layers\n",
    "hidden_layer_1 = layers.Dense(500, activation='relu')(concatted)\n",
    "hidden_layer_2 = layers.Dense(250, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = layers.Dense(50, activation='relu')(hidden_layer_2)\n",
    "hidden_layer_4 = layers.Dense(100, activation='relu')(hidden_layer_3)\n",
    "hidden_layer_5 = layers.Dense(40, activation='relu')(hidden_layer_4)\n",
    "\n",
    "# Outputs\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# output_1 = layers.Dense(1, activation='sigmoid', name=\"is_clicked\", bias_initializer=output_bias)(hidden_layer_5) # If we want to predict \"is_clicked\", use this output (give two outputs to the model instead of one).\n",
    "output_2 = layers.Dense(1, activation=\"sigmoid\", name=\"is_installed\", bias_initializer=output_bias)(hidden_layer_5)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_2])\n",
    "# model = keras.Model(inputs=[cat_input_layer, bin_input_layer, num_input_layer], outputs=[output_1, output_2]) # If we want to predict \"is_clicked\" and \"is_installed\"\n",
    "\n",
    "# Compile model\n",
    "batch_size = 5000 \n",
    "learning_rate=0.001\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset is better to know when to stop training (to do before using the model in production)\n",
    "for i in range(3): # First three training epochs will give good results, after that we would overfit (based on previous tests)\n",
    "    # model.fit(x_norm,y_is_installed, epochs=40, batch_size=5000)\n",
    "    model.fit((cat_selected, bin_selected, num_selected), y_is_installed, epochs = 1, batch_size=batch_size)\n",
    "    model.save(\"models/mv_iterative_embeds/\" + str(i))\n",
    "    y_test = model.predict((cat_test_selected, bin_test_selected, num_test_selected), batch_size=batch_size) \n",
    "\n",
    "    results = pd.DataFrame(test_data[\"f_0\"].copy())\n",
    "    results[\"row_id\"] = results[\"f_0\"]\n",
    "    del results[\"f_0\"]\n",
    "\n",
    "    results[\"is_clicked\"] = 0 # Not predicted here, but can be predicted if the model is configured with two outputs\n",
    "    results[\"is_installed\"] = y_test\n",
    "\n",
    "    save_path=\"results/mv_iterative_embeds/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    results.to_csv(os.path.join(save_path, str(i) + \".csv\"), index=False, header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
